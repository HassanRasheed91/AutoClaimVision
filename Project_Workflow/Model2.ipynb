{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed966e4-08b4-4c1b-a41d-2c6d4bda4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decc52aa-26c2-4a39-a634-8d26e805509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define paths\n",
    "annotations_file = r'C:/Users/ADVAIT/Desktop/CarDD_release/CarDD_release/CarDD_COCO/annotations/instances_train2017.json'  # Adjust the path\n",
    "images_dir =  r'C:/Users/ADVAIT/Desktop/CarDD_release/CarDD_release/CarDD_COCO/train2017'\n",
    "\n",
    "# Load the COCO annotations\n",
    "with open(annotations_file) as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Function to load images and labels\n",
    "def load_data(coco_data):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image in coco_data['images']:\n",
    "        img_path = os.path.join(images_dir, image['file_name'])\n",
    "        images.append(img_path)\n",
    "        # Create a label mapping (this is a simplified example)\n",
    "        img_labels = [ann['category_id'] for ann in coco_data['annotations'] if ann['image_id'] == image['id']]\n",
    "        labels.append(img_labels)\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_data(coco_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91810769-5510-4297-8c0a-53f250c2b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [600, 600])  # Resize to a suitable input size\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Apply preprocessing to your dataset\n",
    "preprocessed_images = [preprocess_image(img) for img in images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3395c4b-f64d-40c9-b76b-c394e55b2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: C:/Users/ADVAIT/Desktop/CrashCal/models/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "# Define the path to the model directory\n",
    "PATH_TO_MODEL_DIR = r\"C:/Users/ADVAIT/Desktop/CrashCal/models/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\"  # Update this line with your folder name\n",
    "\n",
    "# Load the pipeline configuration\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "config_path = f\"{PATH_TO_MODEL_DIR}/pipeline.config\"\n",
    "\n",
    "with tf.io.gfile.GFile(config_path, \"r\") as f:\n",
    "    proto_str = f.read()\n",
    "    text_format.Merge(proto_str, pipeline_config)\n",
    "\n",
    "# Load the model from the saved model directory\n",
    "model_dir = f\"{PATH_TO_MODEL_DIR}/saved_model\"\n",
    "model = tf.saved_model.load(model_dir)\n",
    "\n",
    "# Check the loaded model\n",
    "print(f\"Model loaded from: {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3badc423-aed7-410f-88a8-3c7170e6b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd87ac87-01cd-4612-ac7b-806f767194ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2  # Ensure you have OpenCV installed for image display and saving\n",
    "\n",
    "# Path to your COCO annotations and image directory\n",
    "annotations_file = r'C:/Users/ADVAIT/Desktop/CarDD_release/CarDD_release/CarDD_COCO/annotations/instances_train2017.json'  # Path to annotations file\n",
    "images_dir = r'C:/Users/ADVAIT/Desktop/CarDD_release/CarDD_release/CarDD_COCO/train2017'  # Path to images directory\n",
    "\n",
    "# Path to your model directory and label map\n",
    "PATH_TO_MODEL_DIR = r\"C:/Users/ADVAIT/Desktop/CrashCal/models/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\"  # Model directory path\n",
    "PATH_TO_LABELS = r'C:/Users/ADVAIT/Desktop/CrashCal/label_map.pbtxt'  # Path to the label map file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f56111b-2749-4d58-b9d1-deadd2b68a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [600, 600])  # Resize to a suitable input size\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    image = tf.cast(image * 255.0, tf.uint8)  # Convert back to [0, 255] range and change to uint8\n",
    "    return image\n",
    "\n",
    "# Run inference on images\n",
    "def run_inference(model, image):\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]  # Add batch dimension\n",
    "    output_dict = model(input_tensor)  # Run the image through the model\n",
    "\n",
    "    # Convert outputs to usable formats\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key: value[0, :num_detections].numpy() for key, value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # Handle detection classes and scores\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(int)\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a72597-c5be-428b-8dee-604d79e6912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_damage(severity_score):\n",
    "    if severity_score >= 0.8:\n",
    "        return \"Major\"\n",
    "    elif severity_score >= 0.5:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"Minor\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d15d4d05-27e7-412c-90c1-5b50943552d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image, output_dict, coco_to_custom_mapping, threshold=0.5):\n",
    "    # Define a color map for different classes\n",
    "    color_map = {\n",
    "        1: (0, 255, 0),  # Green for 'tire flat'\n",
    "        3: (255, 0, 0),  # Blue for 'dent'\n",
    "        8: (0, 0, 255),  # Red for 'scratch'\n",
    "        70: (255, 255, 0),  # Cyan for 'crack'\n",
    "        84: (255, 0, 255),  # Magenta for 'glass shatter'\n",
    "        85: (0, 255, 255),  # Yellow for 'lamp broken'\n",
    "        # Add any additional mappings as needed\n",
    "    }\n",
    "\n",
    "    # Filter detections based on confidence threshold\n",
    "    scores = output_dict['detection_scores']\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    classes = output_dict['detection_classes']\n",
    "\n",
    "    # Prepare the image for visualization\n",
    "    image_np = image.numpy() * 255.0  # Convert back to original range\n",
    "    image_np = image_np.astype(np.uint8)  # Convert to uint8 for OpenCV\n",
    "\n",
    "    for i in range(output_dict['num_detections']):\n",
    "        if scores[i] > threshold:  # Only visualize detections above the threshold\n",
    "            box = boxes[i]\n",
    "            class_id = classes[i]\n",
    "            score = scores[i]\n",
    "\n",
    "            # Convert normalized box coordinates to pixel values\n",
    "            h, w, _ = image_np.shape\n",
    "            ymin, xmin, ymax, xmax = (box * np.array([h, w, h, w])).astype(int)\n",
    "\n",
    "            # Determine color for the bounding box\n",
    "            color = color_map.get(class_id, (255, 255, 255))  # Default to white if class ID is not found\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "\n",
    "            # Use mapping to get the category name\n",
    "            category_name = coco_to_custom_mapping.get(class_id, \"Unknown\")\n",
    "            label = f\"{category_name} {score:.2f}\"\n",
    "\n",
    "            # Print label and score to terminal\n",
    "            print(label)  # This will print to the terminal\n",
    "\n",
    "            # Calculate text size and position for centering\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "            text_x = xmin + (xmax - xmin - text_width) // 2  # Center horizontally\n",
    "            text_y = ymin - 10 if ymin - 10 > text_height else ymin + text_height + 5  # Adjust if there's not enough space\n",
    "\n",
    "            # Draw the text centered above the bounding box\n",
    "            cv2.putText(image_np, label, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fdc22c9-f250-4c2d-b1cf-44772467dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crack 0.81\n",
      "dent 0.54\n",
      "dent 0.90\n",
      "scratch 0.79\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m cv2\u001b[38;5;241m.\u001b[39mmoveWindow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected Objects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m(screen_res[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m400\u001b[39m), \u001b[38;5;28mint\u001b[39m(screen_res[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m300\u001b[39m))  \u001b[38;5;66;03m# Center the window\u001b[39;00m\n\u001b[0;32m     33\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected Objects\u001b[39m\u001b[38;5;124m\"\u001b[39m, canvas)  \u001b[38;5;66;03m# Display the canvas with the image\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m key \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Wait for a key press\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# If 'q' is pressed, break the loop\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the mapping from COCO class IDs to your custom categories\n",
    "coco_to_custom_mapping = {\n",
    "    1: 'lamp broken',\n",
    "    3: 'dent',\n",
    "    8: 'scratch',\n",
    "    70: 'crack',\n",
    "    84: 'glass shatter',\n",
    "    85: 'tire flat',\n",
    "    # Add any additional mappings as needed\n",
    "}\n",
    "\n",
    "# Use the new mapping in your visualization function\n",
    "import numpy as np\n",
    "\n",
    "# Loop through images, run inference, and visualize results\n",
    "for img_path in images:\n",
    "    img = preprocess_image(img_path)  # Preprocess the image\n",
    "    output_dict = run_inference(model, img)  # Get predictions\n",
    "    img_with_boxes = visualize_predictions(img, output_dict, coco_to_custom_mapping)  # Visualize the result\n",
    "\n",
    "    # Create a blank canvas for padding\n",
    "    height, width, _ = img_with_boxes.shape\n",
    "    top_margin = 100  # Set your desired top margin\n",
    "    canvas = np.zeros((height + top_margin, width, 3), dtype=np.uint8)  # Create a blank canvas\n",
    "    canvas[top_margin:, :] = img_with_boxes  # Place the image on the canvas\n",
    "\n",
    "    # Create a resizable window and center it\n",
    "    cv2.namedWindow(\"Detected Objects\", cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(\"Detected Objects\", 800, 600)  # Resize as needed\n",
    "    screen_res = (1920, 1080)  # Adjust based on your screen resolution\n",
    "    cv2.moveWindow(\"Detected Objects\", int(screen_res[0]/2 - 400), int(screen_res[1]/2 - 300))  # Center the window\n",
    "\n",
    "    cv2.imshow(\"Detected Objects\", canvas)  # Display the canvas with the image\n",
    "    key = cv2.waitKey(0)  # Wait for a key press\n",
    "    if key == ord('q'):  # If 'q' is pressed, break the loop\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()  # Close all OpenCV windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3abfacf8-b45d-4054-8a5d-72a178115a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch 0.95\n",
      "dent 0.76\n",
      "lamp broken 0.73\n",
      "Unknown 0.66\n",
      "dent 0.56\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your custom image\n",
    "img_path = r'C:/Users/ADVAIT/Desktop/03630.jpg'  # Change this to your image path\n",
    "\n",
    "# Preprocess the image\n",
    "img = preprocess_image(img_path)\n",
    "\n",
    "# Run inference on the image\n",
    "output_dict = run_inference(model, img)\n",
    "\n",
    "# Visualize the results\n",
    "img_with_boxes = visualize_predictions(img, output_dict, coco_to_custom_mapping)\n",
    "\n",
    "# Create a blank canvas for padding\n",
    "height, width, _ = img_with_boxes.shape\n",
    "top_margin = 100  # Set your desired top margin\n",
    "canvas = np.zeros((height + top_margin, width, 3), dtype=np.uint8)  # Create a blank canvas\n",
    "canvas[top_margin:, :] = img_with_boxes  # Place the image on the canvas\n",
    "\n",
    "# Create a resizable window and center it\n",
    "cv2.namedWindow(\"Detected Objects\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Detected Objects\", 800, 600)  # Resize as needed\n",
    "screen_res = (1920, 1080)  # Adjust based on your screen resolution\n",
    "cv2.moveWindow(\"Detected Objects\", int(screen_res[0]/2 - 400), int(screen_res[1]/2 - 300))  # Center the window\n",
    "\n",
    "# Display the canvas with the image\n",
    "cv2.imshow(\"Detected Objects\", canvas)\n",
    "cv2.waitKey(0)  # Wait for a key press\n",
    "cv2.destroyAllWindows()  # Close all OpenCV windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec272754-8d7e-4d86-9971-ff017096647e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f180a8-eda5-40e7-baf4-67119dbcb36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
